Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Kingma2015,
abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions. The method is straightforward to implement and is based on adaptive estimates of lower-order moments of the gradients. The method is computationally efficient, has little memory requirements and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The method exhibits invariance to diagonal rescaling of the gradients by adapting to the geometry of the objective function. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. We demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods.},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, Diederik P. and Ba, Jimmy Lei},
eprint = {1412.6980},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Ba - 2015 - Adam a Method for Stochastic Optimization.pdf:pdf},
journal = {International Conference on Learning Representations 2015},
pages = {1--15},
title = {{Adam: a Method for Stochastic Optimization}},
year = {2015}
}
@inproceedings{Hochreiter1995,
author = {Hochreiter, S and Schmidhuber, J},
booktitle = {Advances in Neural Information Processing Systems 7 (NIPS 7)},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Schmidhuber - 1995 - Simplifying neural nets by discovering flat minima.pdf:pdf},
keywords = {juergen},
pages = {529--536},
title = {{Simplifying neural nets by discovering flat minima}},
year = {1995}
}
@article{Rezende2014,
abstract = {Abstract We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep , directed generative models , endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition ... $\backslash$n},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.4082v3},
author = {Rezende, D J and Mohamed, S and Wierstra, D},
eprint = {arXiv:1401.4082v3},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Rezende, Mohamed, Wierstra - 2014 - Stochastic backpropagation and approximate inference in deep generative models.pdf:pdf},
isbn = {9781634393973},
journal = {Proceedings of The 31st {\ldots}},
pages = {1278--1286},
title = {{Stochastic backpropagation and approximate inference in deep generative models}},
url = {http://jmlr.org/proceedings/papers/v32/rezende14.html{\%}5Cnpapers3://publication/uuid/F2747569-7719-4EAC-A5A7-9ECA9D6A8FE6},
volume = {32},
year = {2014}
}
@article{Merity2016,
abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
archivePrefix = {arXiv},
arxivId = {1609.07843},
author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
eprint = {1609.07843},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Merity et al. - 2016 - Pointer Sentinel Mixture Models.pdf:pdf},
journal = {Arxiv},
title = {{Pointer Sentinel Mixture Models}},
url = {http://arxiv.org/abs/1609.07843},
year = {2016}
}
@article{Marcus1993,
abstract = {There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenom- ena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large cor- pora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valu- able for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investi- gation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models.},
author = {Marcus, Mitchell P and Santorini, Beatrice and Marcinkiewicz, Mary Ann},
doi = {10.1162/coli.2010.36.1.36100},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Marcus, Santorini, Marcinkiewicz - 1993 - Building a large annotated corpus of English The Penn Treebank.pdf:pdf},
isbn = {0891-2017},
issn = {08912017},
journal = {Computational Linguistics},
keywords = {POS-Tagging},
number = {2},
pages = {313--330},
title = {{Building a large annotated corpus of English: The Penn Treebank.}},
volume = {19},
year = {1993}
}
@article{Amodei2015,
abstract = {We show that an end-to-end deep learning approach can be used to recognize either English or Mandarin Chinese speech—two vastly different languages. Be- cause it replaces entire pipelines of hand-engineered components with neural net- works, end-to-end learning allows us to handle a diverse variety of speech includ- ing noisy environments, accents and different languages. Key to our approach is our application of HPC techniques, resulting in a 7x speedup over our previous system [26]. Because of this efficiency, experiments that previously took weeks now run in days. This enables us to iterate more quickly to identify superior ar- chitectures and algorithms. As a result, in several cases, our system is competitive with the transcription of human workers when benchmarked on standard datasets. Finally, using a technique called Batch Dispatch with GPUs in the data center, we show that our system can be inexpensively deployed in an online setting, deliver- ing low latency when serving users at scale.},
archivePrefix = {arXiv},
arxivId = {1512.02595},
author = {Amodei, Dario and Anubhai, Rishita and Battenberg, Eric and Carl, Case and Casper, Jared and Catanzaro, Bryan and Chen, Jingdong and Chrzanowski, Mike and Coates, Adam and Diamos, Greg and Elsen, Erich and Engel, Jesse and Fan, Linxi and Fougner, Christopher and Han, Tony and Hannun, Awni and Jun, Billy and LeGresley, Patrick and Lin, Libby and Narang, Sharan and Ng, Andrew and Ozair, Sherjil and Prenger, Ryan and Raiman, Jonathan and Satheesh, Sanjeev and Seetapun, David and Sengupta, Shubho and Wang, Yi and Wang, Zhiqian and Wang, Chong and Xiao, Bo and Yogatama, Dani and Zhan, Jun and Zhu, Zhenyao},
doi = {10.1145/1143844.1143891},
eprint = {1512.02595},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Amodei et al. - 2015 - Deep-speech 2 End-to-end speech recognition in English and Mandarin.pdf:pdf},
isbn = {9781510829008},
issn = {10987576},
journal = {Jmlr W{\&}Cp},
pages = {28},
pmid = {1000285842},
title = {{Deep-speech 2: End-to-end speech recognition in English and Mandarin}},
volume = {48},
year = {2015}
}
@article{Graves2016,
abstract = {The ability to backpropagate stochastic gradients through continuous latent distributions has been crucial to the emergence of variational autoencoders and stochastic gradient variational Bayes. The key ingredient is an unbiased and low-variance way of estimating gradients with respect to distribution parameters from gradients evaluated at distribution samples. The "reparameterization trick" provides a class of transforms yielding such estimators for many continuous distributions, including the Gaussian and other members of the location-scale family. However the trick does not readily extend to mixture density models, due to the difficulty of reparameterizing the discrete distribution over mixture weights. This report describes an alternative transform, applicable to any continuous multivariate distribution with a differentiable density function from which samples can be drawn, and uses it to derive an unbiased estimator for mixture density weight derivatives. Combined with the reparameterization trick applied to the individual mixture components, this estimator makes it straightforward to train variational autoencoders with mixture-distributed latent variables, or to perform stochastic variational inference with a mixture density variational posterior.},
archivePrefix = {arXiv},
arxivId = {1607.05690},
author = {Graves, Alex},
eprint = {1607.05690},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Graves - 2016 - Stochastic Backpropagation through Mixture Density Distributions.pdf:pdf},
number = {1},
pages = {1--6},
title = {{Stochastic Backpropagation through Mixture Density Distributions}},
url = {http://arxiv.org/abs/1607.05690},
volume = {1},
year = {2016}
}
@article{Wu2016,
abstract = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units ("wordpieces") for both input and output. This method provides a good balance between the flexibility of "character"-delimited models and the efficiency of "word"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60{\%} compared to Google's phrase-based production system.},
archivePrefix = {arXiv},
arxivId = {1609.08144v2},
author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V. and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and Klingner, Jeff and Shah, Apurva and Johnson, Melvin and Liu, Xiaobing and Kaiser, {\L}ukasz and Gouws, Stephan and Kato, Yoshikiyo and Kudo, Taku and Kazawa, Hideto and Stevens, Keith and Kurian, George and Patil, Nishant and Wang, Wei and Young, Cliff and Smith, Jason and Riesa, Jason and Rudnick, Alex and Vinyals, Oriol and Corrado, Greg and Hughes, Macduff and Dean, Jeffrey},
eprint = {1609.08144v2},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Wu et al. - 2016 - Google's Neural Machine Translation System Bridging the Gap between Human and Machine Translation.pdf:pdf},
journal = {ArXiv e-prints},
keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Learning},
pages = {1--23},
title = {{Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation}},
url = {http://arxiv.org/abs/1609.08144},
year = {2016}
}
@article{Soudry2014,
abstract = {Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expec- tation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a “mean-field” factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical ap- proximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. Despite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several addi- tional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where param- eter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision lim- ited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. We test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior.},
author = {Soudry, D and Hubara, I and Meir, R},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Soudry, Hubara, Meir - 2014 - Expectation Backpropagation parameter-free training of multilayer neural networks with real and discrete w.pdf:pdf},
issn = {10495258},
journal = {Neural Information Processing Systems 2014},
number = {1},
pages = {1--9},
title = {{Expectation Backpropagation: parameter-free training of multilayer neural networks with real and discrete weights}},
volume = {2},
year = {2014}
}
@article{Lipton2016,
abstract = {When rewards are sparse and efficient exploration essential, deep Q-learning with {\$}\backslashepsilon{\$}-greedy exploration tends to fail. This poses problems for otherwise promising domains such as task-oriented dialog systems, where the primary reward signal, indicating successful completion, typically occurs only at the end of each episode but depends on the entire sequence of utterances. A poor agent encounters such successful dialogs rarely, and a random agent may never stumble upon a successful outcome in reasonable time. We present two techniques that significantly improve the efficiency of exploration for deep Q-learning agents in dialog systems. First, we demonstrate that exploration by Thompson sampling, using Monte Carlo samples from a Bayes-by-Backprop neural network, yields marked improvement over standard DQNs with Boltzmann or {\$}\backslashepsilon{\$}-greedy exploration. Second, we show that spiking the replay buffer with a small number of successes, as are easy to harvest for dialog tasks, can make Q-learning feasible when it might otherwise fail catastrophically.},
archivePrefix = {arXiv},
arxivId = {1608.05081},
author = {Lipton, Zachary C and Gao, Jianfeng and Li, Lihong and Li, Xiujun and Ahmed, Faisal and Deng, Li},
eprint = {1608.05081},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Lipton et al. - 2016 - Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks and Replay Buffer Spiking.pdf:pdf},
journal = {arXiv:1608.05081v1 [cs.LG]},
pages = {1--16},
title = {{Efficient Exploration for Dialog Policy Learning with Deep BBQ Networks and Replay Buffer Spiking}},
url = {http://arxiv.org/abs/1608.05081},
year = {2016}
}
@article{Vinyals2016,
abstract = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset.We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
archivePrefix = {arXiv},
arxivId = {1609.06647},
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
doi = {10.1109/TPAMI.2016.2587640},
eprint = {1609.06647},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Vinyals et al. - 2016 - Show and Tell Lessons learned from the 2015 MSCOCO Image Captioning Challenge.pdf:pdf},
isbn = {0162-8828 VO - PP},
issn = {0162-8828},
journal = {Tpami},
number = {PP},
pages = {1--1},
pmid = {27392341},
title = {{Show and Tell: Lessons learned from the 2015 MSCOCO Image Captioning Challenge}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7505636},
volume = {99},
year = {2016}
}
@article{Gan2016,
abstract = {Recurrent neural networks (RNNs) have shown promising performance for lan-guage modeling. However, traditional training of RNNs using back-propagation through time often suffers from overfit-ting. One reason for this is that stochastic optimization (used for large training sets) does not provide good estimates of model uncertainty. This paper leverages recent advances in stochastic gradient Markov Chain Monte Carlo (also appropriate for large training sets) to learn weight uncer-tainty in RNNs. It yields a principled Bayesian learning algorithm, adding gra-dient noise during training (enhancing ex-ploration of the model-parameter space) and model averaging when testing. Ex-tensive experiments on various RNN mod-els and across a broad range of applica-tions demonstrate the superiority of the proposed approach over stochastic opti-mization.},
archivePrefix = {arXiv},
arxivId = {1611.08034},
author = {Gan, Zhe and Li, Chunyuan and Chen, Changyou and Pu, Yunchen and Su, Qinliang and Carin, Lawrence},
eprint = {1611.08034},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Gan et al. - 2016 - Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling.pdf:pdf},
journal = {arXiv preprint},
title = {{Scalable Bayesian Learning of Recurrent Neural Networks for Language Modeling}},
year = {2016}
}
@article{Welling2011,
abstract = {In this paper we propose a new framework for learning from large scale datasets based on iterative learning from small mini-batches. By adding the right amount of noise to a standard stochastic gradient optimization algorithm we show that the iterates will converge to samples from the true posterior distribution as we anneal the stepsize. This seamless transition between optimization and Bayesian posterior sampling provides an in-built protection against overfitting. We also propose a practical method for Monte Carlo estimates of posterior statistics which monitors a ``sampling threshold'' and collects samples after it has been surpassed. We apply the method to three models: a mixture of Gaussians, logistic regression and ICA with natural gradients.},
archivePrefix = {arXiv},
arxivId = {arXiv:1203.5753v5},
author = {Welling, M. and Teh, Y.-W.},
eprint = {arXiv:1203.5753v5},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Welling, Teh - 2011 - Bayesian Learning via Stochastic Gradient Langevin Dynamics.pdf:pdf},
isbn = {978-1-4503-0619-5},
journal = {Proceedings of the 28th International Conference on Machine Learning},
keywords = {Bayesian learning,ICML,machine learning,online learning},
pages = {681--688},
title = {{Bayesian Learning via Stochastic Gradient Langevin Dynamics}},
year = {2011}
}
@article{Bayer2014,
abstract = {Leveraging advances in variational inference, we propose to enhance recurrent neural networks with latent variables, resulting in Stochastic Recurrent Networks (STORNs). The model i) can be trained with stochastic gradient methods, ii) allows structured and multi-modal conditionals at each time step, iii) features a reliable estimator of the marginal likelihood and iv) is a generalisation of deterministic recurrent neural networks. We evaluate the method on four polyphonic musical data sets and motion capture data.},
archivePrefix = {arXiv},
arxivId = {1411.7610},
author = {Bayer, Justin and Osendorfer, Christian},
eprint = {1411.7610},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Bayer, Osendorfer - 2014 - Learning Stochastic Recurrent Networks.pdf:pdf},
pages = {1--9},
title = {{Learning Stochastic Recurrent Networks}},
url = {http://arxiv.org/abs/1411.7610},
year = {2014}
}
@article{Adams2015,
abstract = {Large multilayer neural networks trained with backpropagation have recently achieved state-of-the-art results in a wide range of problems. However, using backprop for neural net learning still has some disadvantages, e.g., having to tune a large number of hyperparameters to the data, lack of calibrated probabilistic predictions, and a tendency to overfit the training data. In principle, the Bayesian approach to learning neural networks does not have these problems. However, existing Bayesian techniques lack scalability to large dataset and network sizes. In this work we present a novel scalable method for learning Bayesian neural networks, called probabilistic backpropagation (PBP). Similar to classical backpropagation, PBP works by computing a forward propagation of probabilities through the network and then doing a backward computation of gradients. A series of experiments on ten real-world datasets show that PBP is significantly faster than other techniques, while offering competitive predictive abilities. Our experiments also show that PBP provides accurate estimates of the posterior variance on the network weights.},
archivePrefix = {arXiv},
arxivId = {1502.05336},
author = {Adams, Ryan P},
eprint = {1502.05336},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Adams - 2015 - Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks.pdf:pdf},
isbn = {9781510810587},
journal = {Journal of Machine Learning Research},
pages = {1--6},
title = {{Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks}},
volume = {37},
year = {2015}
}
@article{Jozefowicz2016,
abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 24.2. We also release these models for the NLP and ML community to study and improve upon.},
archivePrefix = {arXiv},
arxivId = {1602.02410},
author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
eprint = {1602.02410},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Jozefowicz et al. - 2016 - Exploring the Limits of Language Modeling.pdf:pdf},
journal = {arXiv:1602.02410 [cs]},
title = {{Exploring the Limits of Language Modeling}},
url = {http://arxiv.org/abs/1602.02410{\%}5Cnhttp://www.arxiv.org/pdf/1602.02410.pdf},
year = {2016}
}
@article{Zilly2016,
abstract = {Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with such 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which are long not only in time but also in space, generalizing LSTMs to larger step-to-step depths. Experiments indicate that the proposed architecture results in complex but efficient models, beating previous models for character prediction on the Hutter Prize Wikipedia dataset and word-level language modeling on the Penn Treebank corpus.},
archivePrefix = {arXiv},
arxivId = {1607.03474},
author = {Zilly, Julian Georg and Srivastava, Rupesh Kumar and Koutn{\'{i}}k, Jan and Schmidhuber, J{\"{u}}rgen},
eprint = {1607.03474},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Zilly et al. - 2016 - Recurrent Highway Networks.pdf:pdf},
journal = {arXiv preprint},
pages = {11},
title = {{Recurrent Highway Networks}},
url = {http://arxiv.org/abs/1607.03474},
year = {2016}
}
@article{Gal2015,
abstract = {Deep learning tools have recently gained much attention in applied machine learn- ing. However such tools for regression and classification do not allow us to cap- ture model uncertainty. Bayesian models offer us the ability to reason about model uncertainty, but usually come with a prohibitive computational cost. We show that dropout in multilayer perceptron models (MLPs) can be interpreted as a Bayesian approximation. Results are obtained for modelling uncertainty for dropout MLP models – extracting information that has been thrown away so far, from existing models. This mitigates the problem of representing uncertainty in deep learning without sacrificing computational performance or test accuracy. We perform an exploratory study of the dropout uncertainty properties. Various network architectures and non-linearities are assessed on tasks of extrapolation, interpolation, and classification. We show that model uncertainty is important for classification tasks using MNIST as an example, and use the model's uncertainty in a Bayesian pipeline, with deep reinforcement learning as a concrete example.},
archivePrefix = {arXiv},
arxivId = {arXiv:1506.02142v1},
author = {Gal, Yarin and Ghahramani, Zoubin},
eprint = {arXiv:1506.02142v1},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Gal, Ghahramani - 2015 - Dropout as a Bayesian Approximation Representing Model Uncertainty in Deep Learning.pdf:pdf},
isbn = {1506.02142},
journal = {Icml},
pages = {1--10},
pmid = {88045},
title = {{Dropout as a Bayesian Approximation : Representing Model Uncertainty in Deep Learning}},
volume = {48},
year = {2015}
}
@article{Hinton1993,
abstract = {Supervised neural networks generalize there is much less information learning, well if in the weights than there is in the output vectors of the train- ing cases. So during it is impor- tant to keep the weights simple by penaliz- ing the amount of information The amount of information be controlled they contain. in a weight can by adding Gaussian noise and the noise level can be adapted during learning to optimize a method of computing the trade-off between the expected squared error of the network and the amount of information in the noisy weights can be computed is required in the weights. We describe the derivatives work that contains a layer of non-linear time-consuming that of the expected squared error and of the amount of information in a net- hidden units. Provided the output units are linear, the exact derivatives without efficiently tions. The idea of minimizing information Monte Carlo simula- the amount of to communicate the weights of a neural network leads to a number of intereating schemes for encoding the weights.},
author = {Hinton, Geoffrey E. and Hinton, Geoffrey E. and van Camp, Drew and van Camp, Drew},
doi = {10.1145/168304.168306},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Hinton et al. - 1993 - Keeping the neural networks simple by minimizing the description length of the weights.pdf:pdf},
isbn = {0897916115},
journal = {Proceedings of the sixth annual conference on Computational learning theory - COLT '93},
pages = {5--13},
title = {{Keeping the neural networks simple by minimizing the description length of the weights}},
url = {http://portal.acm.org/citation.cfm?doid=168304.168306},
year = {1993}
}
@article{Lin2014,
abstract = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
archivePrefix = {arXiv},
arxivId = {arXiv:1405.0312v1},
author = {Lin, Tsung Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'{a}}r, Piotr and Zitnick, C. Lawrence},
doi = {10.1007/978-3-319-10602-1_48},
eprint = {arXiv:1405.0312v1},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Lin et al. - 2014 - Microsoft COCO Common objects in context.pdf:pdf},
isbn = {978-3-319-10601-4},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 5},
pages = {740--755},
title = {{Microsoft COCO: Common objects in context}},
volume = {8693 LNCS},
year = {2014}
}
@book{Wainwright2008,
abstract = {The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statisti- cal, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimiza- tion, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representa- tions of the problems of computing likelihoods, marginal probabili- ties and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational meth- ods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relax- ations—can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.},
author = {Wainwright, MJ},
booktitle = {Foundations and Trends{\textregistered} in Machine Learning},
doi = {10.1561/2200000001},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Wainwright - 2008 - Graphical models, exponential families, and variational inference.pdf:pdf},
isbn = {9781601981844},
issn = {1935-8237},
number = {1–2},
pages = {1--305},
title = {{Graphical models, exponential families, and variational inference}},
url = {http://www.nowpublishers.com/product.aspx?product=MAL{\&}doi=2200000001{\%}5Cnhttp://dl.acm.org/citation.cfm?id=1498841},
volume = {1},
year = {2008}
}
@article{Houthooft2016,
abstract = {Scalable and effective exploration remains a key challenge in reinforcement learn-ing (RL). While there are methods with optimality guarantees in the setting of dis-crete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an explo-ration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using varia-tional inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
archivePrefix = {arXiv},
arxivId = {1605.09674},
author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and {De Turck}, Filip and Abbeel, Pieter},
eprint = {1605.09674},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Houthooft et al. - 2016 - Variational Information Maximizing Exploration.pdf:pdf},
journal = {Advances in Neural Information Processing Systems (NIPS)},
title = {{Variational Information Maximizing Exploration}},
year = {2016}
}
@article{Li2016a,
abstract = {Effective training of deep neural networks suffers from two main issues. The first is that the parameter spaces of these models exhibit pathological curvature. Recent methods address this problem by using adaptive preconditioning for Stochastic Gradient Descent (SGD). These methods improve convergence by adapting to the local geometry of parameter space. A second issue is overfitting, which is typically addressed by early stopping. However, recent work has demonstrated that Bayesian model averaging mitigates this problem. The posterior can be sampled by using Stochastic Gradient Langevin Dynamics (SGLD). However, the rapidly changing curvature renders default SGLD methods inefficient. Here, we propose combining adaptive preconditioners with SGLD. In support of this idea, we give theoretical properties on asymptotic convergence and predictive risk.We also provide empirical results for Logistic Regression, Feedforward Neural Nets, and Convolutional Neural Nets, demonstrating that our preconditioned SGLD method gives state-of-the-art performance on these models.},
archivePrefix = {arXiv},
arxivId = {1512.07666},
author = {Li, Chunyuan and Chen, Changyou and Carlson, David and Carin., Lawrence},
eprint = {1512.07666},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2016 - Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks.pdf:pdf},
isbn = {9781577357605},
journal = {Proceedings of the 30th Conference on Artificial Intelligence (AAAI 2016)},
keywords = {Precondition,SGD},
pages = {1788--1794},
title = {{Preconditioned Stochastic Gradient Langevin Dynamics for Deep Neural Networks}},
year = {2016}
}
@article{Kim2016,
abstract = {We describe a simple neural language model that relies only on character-level inputs. Predictions are still made at the word-level. Our model employs a convolutional neural network (CNN) and a highway network over characters, whose output is given to a long short-term memory (LSTM) recurrent neural network language model (RNN-LM). On the English Penn Treebank the model is on par with the existing state-of-the-art despite having 60{\%} fewer parameters. On languages with rich morphology (Czech, German, French, Spanish, Russian), the model consistently outperforms a Kneser-Ney baseline and word-level/morpheme-level LSTM baselines, again with far fewer parameters. Our results suggest that on many languages, character inputs are sufficient for language modeling.},
archivePrefix = {arXiv},
arxivId = {1508.06615},
author = {Kim, Yoon and Jernite, Yacine and Sontag, David and Rush, Alexander M.},
eprint = {1508.06615},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Kim et al. - 2016 - Character-Aware Neural Language Models.pdf:pdf},
isbn = {9781577357605},
journal = {Aaai},
keywords = {Technical Papers: Natural Language Processing and},
pages = {2741--2749},
title = {{Character-Aware Neural Language Models}},
url = {http://arxiv.org/abs/1508.06615},
year = {2016}
}
@book{Mackay1995,
abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well-matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. This review describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Mackay, David},
booktitle = {Network: Computation in Neural Systems},
doi = {10.1088/0954-898X/6/3/011},
eprint = {9809069v1},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Mackay - 1995 - Probable networks and plausible predictions — a review of practical Bayesian methods for supervised neural networks.pdf:pdf},
isbn = {0000000000},
issn = {0954-898X},
pages = {469--505},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Probable networks and plausible predictions — a review of practical Bayesian methods for supervised neural networks}},
volume = {6},
year = {1995}
}
@article{Li2016b,
abstract = {Algorithm design is a laborious process and often requires many iterations of ideation and validation. In this paper, we explore automating algorithm design and present a method to learn an optimization algorithm, which we believe to be the first method that can automatically discover a better algorithm. We approach this problem from a reinforcement learning perspective and represent any particular optimization algorithm as a policy. We learn an optimization algorithm using guided policy search and demonstrate that the resulting algorithm outperforms existing hand-engineered algorithms in terms of convergence speed and/or the final objective value.},
archivePrefix = {arXiv},
arxivId = {1606.01885},
author = {Li, Ke and Malik, Jitendra},
eprint = {1606.01885},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Li, Malik - 2016 - Learning to Optimize.pdf:pdf},
title = {{Learning to Optimize}},
url = {http://arxiv.org/abs/1606.01885},
year = {2016}
}
@article{Liu2016,
abstract = {In this paper, we propose a novel training procedure for image captioning models based on policy gradient methods. This allows us to directly optimize for the metrics of interest, rather than just maximizing likelihood of human generated captions. We show that by optimizing for standard metrics such as BLEU, CIDEr, METEOR and ROUGE, we can develop a system that improve on the metrics and ranks first on the MSCOCO image captioning leader board, even though our CNN-RNN model is much simpler than state of the art models. We further show that by also optimizing for the recently introduced SPICE metric, which measures semantic quality of captions, we can produce a system that significantly outperforms other methods as measured by human evaluation. Finally, we show how we can leverage extra sources of information, such as pre-trained image tagging models, to further improve quality.},
archivePrefix = {arXiv},
arxivId = {1612.00370},
author = {Liu, Siqi and Zhu, Zhenhai and Ye, Ning and Guadarrama, Sergio and Murphy, Kevin},
eprint = {1612.00370},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Liu et al. - 2016 - Optimization of image description metrics using policy gradient methods.pdf:pdf},
journal = {1612.00370V1},
title = {{Optimization of image description metrics using policy gradient methods}},
year = {2016}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Ghahramani2015,
abstract = {How can a machine learn from experience? Probabilistic modelling provides a framework for understanding what learning is, and has therefore emerged as one of the principal theoretical and practical approaches for designing machines that learn from data acquired through experience. The probabilistic framework, which describes how to represent and manipulate uncertainty about models and predictions, has a central role in scientific data analysis, machine learning, robotics, cognitive science and artificial intelligence. This Review provides an introduction to this framework, and discusses some of the state-of-the-art advances in the field, namely, probabilistic programming, Bayesian optimization, data compression and automatic model discovery.},
author = {Ghahramani, Zoubin},
issn = {0028-0836},
journal = {Nature},
month = {may},
number = {7553},
pages = {452--459},
publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
title = {{Probabilistic machine learning and artificial intelligence}},
url = {http://dx.doi.org/10.1038/nature14541 http://10.0.4.14/nature14541},
volume = {521},
year = {2015}
}
@article{Tokui2015,
abstract = {A long strand of empirical research has claimed that dropout cannot be applied between the recurrent connections of a recurrent neural network (RNN). The reasoning has been that the noise hinders the network's ability to model sequences, and instead should be applied to the RNN's inputs and outputs alone. But dropout is a vital tool for regularisation, and without dropout in recurrent layers our models overfit quickly. In this paper we show that a recently developed theoretical framework, casting dropout as approximate Bayesian inference, can give us mathematically grounded tools to apply dropout within the recurrent layers. We apply our new dropout technique in long short-term memory (LSTM) networks and show that the new approach significantly outperforms existing techniques.},
archivePrefix = {arXiv},
arxivId = {1512.05287},
author = {Tokui, Seiya and Networks, Preferred},
doi = {10.1201/9781420049176},
eprint = {1512.05287},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Tokui, Networks - 2015 - A Theoretically Grounded Application of Dropout in Recurrent Neural Networks.pdf:pdf},
isbn = {9789537619084},
issn = {0302-9743},
journal = {arXiv:1512.05287},
pages = {1--9},
title = {{A Theoretically Grounded Application of Dropout in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.05287},
year = {2015}
}
@article{Hochreiter1997,
annote = {NULL},
author = {{Hochreiter, Sepp; Schmidhuber}, Jurgen},
doi = {10.1144/GSL.MEM.1999.018.01.02},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Hochreiter, Sepp Schmidhuber - 1997 - Long short-term memory.pdf:pdf},
isbn = {9781457711022},
issn = {0435-4052},
journal = {Neural Computation},
keywords = {LSTM Intro,Previous Work},
mendeley-tags = {LSTM Intro,Previous Work},
number = {8},
pages = {1--32},
title = {{Long short-term memory}},
volume = {9},
year = {1997}
}
@article{Chung2015,
abstract = {In this paper, we explore the inclusion of random variables into the dynamic latent state of a recurrent neural network (RNN) by combining elements of the variational autoencoder. We argue that through the use of high-level latent random variables, our variational RNN (VRNN) is able to learn to model the kind of variability observed in highly-structured sequential data (such as speech). We empirically evaluate the proposed model against related sequential models on five sequence datasets, four of speech and one of handwriting. Our results show the importance of the role random variables can play in the RNN dynamic latent state.},
archivePrefix = {arXiv},
arxivId = {1506.02216},
author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron and Bengio, Yoshua},
eprint = {1506.02216},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Chung et al. - 2015 - A Recurrent Latent Variable Model for Sequential Data.pdf:pdf},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 28 (NIPS 2015)},
pages = {8},
title = {{A Recurrent Latent Variable Model for Sequential Data}},
url = {http://arxiv.org/abs/1506.02216},
year = {2015}
}
@article{Blundell2015a,
abstract = {We introduce a new, efficient, principled and backpropagation-compatible algorithm for learning a probability distribution on the weights of a neural network, called Bayes by Backprop. It regularises the weights by minimising a compression cost, known as the variational free energy or the expected lower bound on the marginal likelihood. We show that this principled kind of regularisation yields comparable performance to dropout on MNIST classification. We then demonstrate how the learnt uncertainty in the weights can be used to improve generalisation in non-linear regression problems, and how this weight uncertainty can be used to drive the exploration-exploitation trade-off in reinforcement learning.},
archivePrefix = {arXiv},
arxivId = {1505.05424v2},
author = {Blundell, Charles and Cornebise, Julien and Kavukcuoglu, Koray and Wierstra, Daan},
eprint = {1505.05424v2},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Blundell et al. - 2015 - Weight Uncertainty in Neural Networks.pdf:pdf},
isbn = {9781510810587},
journal = {Icml},
pages = {1613--1622},
title = {{Weight Uncertainty in Neural Networks}},
url = {http://arxiv.org/abs/1505.05424{\%}5Cnhttp://www.arxiv.org/pdf/1505.05424.pdf},
volume = {37},
year = {2015}
}
@article{Wang2013,
abstract = {Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (Hinton et al., 2012) does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformations.},
author = {Wang, Sida I and Manning, Christopher D},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Wang, Manning - 2013 - Fast dropout training.pdf:pdf},
journal = {Proceedings of the 30th International Conference on Machine Learning},
keywords = {I,boring formatting information,machine learning},
pages = {118--126},
title = {{Fast dropout training}},
url = {http://machinelearning.wustl.edu/mlpapers/papers/wang13a},
volume = {28},
year = {2013}
}
@article{Duchi2011,
abstract = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms.},
archivePrefix = {arXiv},
arxivId = {arXiv:1103.4296v1},
author = {Duchi, John and Hazan, Elad and Singer, Yoram},
doi = {10.1109/CDC.2012.6426698},
eprint = {arXiv:1103.4296v1},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Duchi, Hazan, Singer - 2011 - Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:pdf},
isbn = {9780982252925},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {adaptivity,online learning,stochastic convex optimization,subgradient methods},
pages = {2121--2159},
pmid = {2868127},
title = {{Adaptive Subgradient Methods for Online Learning and Stochastic Optimization}},
url = {http://jmlr.org/papers/v12/duchi11a.html},
volume = {12},
year = {2011}
}
@article{Mikolov2010,
abstract = {基于RNN模型的语言模型，详细可参考作者的博士论文。周期神经网络。但是上下文，也没用取全部的，只取到了前5个。},
author = {Mikolov, T and Karafiat, M and Burget, L and Cernocky, J and Khudanpur, S},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2010 - Recurrent Neural Network based Language Model.pdf:pdf},
journal = {Interspeech},
number = {September},
pages = {1045--1048},
title = {{Recurrent Neural Network based Language Model}},
year = {2010}
}
@article{Lu2016,
abstract = {Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as "the" and "of". Other words that may seem visual can often be predicted reliably just from the language model e.g., "sign" after "behind a red stop" or "phone" following "talking on a cell". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.},
archivePrefix = {arXiv},
arxivId = {1612.01887},
author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
eprint = {1612.01887},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Lu et al. - 2016 - Knowing When to Look Adaptive Attention via A Visual Sentinel for Image Captioning.pdf:pdf},
journal = {1612.01887V1},
title = {{Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning}},
url = {http://arxiv.org/abs/1612.01887},
year = {2016}
}
@article{Kingma2013a,
abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
archivePrefix = {arXiv},
arxivId = {1312.6114},
author = {Kingma, Diederik P and Welling, Max},
doi = {10.1051/0004-6361/201527329},
eprint = {1312.6114},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Kingma, Welling - 2013 - Auto-Encoding Variational Bayes.pdf:pdf},
isbn = {1312.6114v10},
issn = {1312.6114v10},
number = {Ml},
pages = {1--14},
title = {{Auto-Encoding Variational Bayes}},
url = {http://arxiv.org/abs/1312.6114},
year = {2013}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Zaremba, Sutskever, Vinyals - 2014 - Recurrent Neural Network Regularization.pdf:pdf},
isbn = {078036404X},
issn = {0157244X},
journal = {Iclr},
keywords = {Natural Language Processing, Recurrent Neural Netw,Recurrent Neural Netw},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Teh2015,
abstract = {This paper makes two contributions to Bayesian machine learning algorithms. Firstly, we propose stochastic natural gradient expectation propagation (SNEP), a novel alternative to expectation propagation (EP), a popular variational inference algorithm. SNEP is a black box variational algorithm, in that it does not require any simplifying assumptions on the distribution of interest, beyond the existence of some Monte Carlo sampler for estimating the moments of the EP tilted distributions. Further, as opposed to EP which has no guarantee of convergence, SNEP can be shown to be convergent, even when using Monte Carlo moment estimates. Secondly, we propose a novel architecture for distributed Bayesian learning which we call the posterior server. The posterior server allows scalable and robust Bayesian learning in cases where a dataset is stored in a distributed manner across a cluster, with each compute node containing a disjoint subset of data. An independent Markov chain Monte Carlo (MCMC) sampler is run on each compute node, with direct access only to the local data subset, but which targets an approximation to the global posterior distribution given all data across the whole cluster. This is achieved by using a distributed asynchronous implementation of SNEP to pass messages across the cluster. We demonstrate SNEP and the posterior server on distributed Bayesian learning of logistic regression and neural networks.},
archivePrefix = {arXiv},
arxivId = {1512.09327},
author = {Teh, Yee Whye and Hasenclever, Leonard and Lienart, Thibaut and Vollmer, Sebastian and Webb, Stefan and Lakshminarayanan, Balaji and Blundell, Charles},
eprint = {1512.09327},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Teh et al. - 2015 - Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server.pdf:pdf},
isbn = {1512.09327},
journal = {arXiv:1512.09327 [cs, stat]},
keywords = {distributed bayesian learning,expectation propagation,large scale learning,markov chain monte carlo,mation,natural gradient,posterior server,stochastic approxi-},
pages = {1--30},
title = {{Distributed Bayesian Learning with Stochastic Natural-gradient Expectation Propagation and the Posterior Server}},
url = {http://arxiv.org/abs/1512.09327{\%}5Cnhttp://www.arxiv.org/pdf/1512.09327.pdf},
year = {2015}
}
@article{Molchanov2014,
abstract = {We explore recently proposed variational dropout technique which provided an elegant Bayesian interpretation to dropout. We extend variational dropout to the case when dropout rate is unknown and show that it can be found by optimizing evidence variational lower bound. We show that it is possible to assign and find individual dropout rates to each connection in DNN. Interestingly such assignment leads to ex-tremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination (ARD) effect in empirical Bayes but has a number of advantages. We report up to 128 fold compres-sion of popular architectures without a large loss of accuracy providing additional evidence to the fact that modern deep architectures are very redundant.},
archivePrefix = {arXiv},
arxivId = {1701.05369},
author = {Molchanov, Dmitry and Ru, Dmitry Molchanov@skolkovotech and Ashukha, Arsenii and Vetrov, Dmitry},
eprint = {1701.05369},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Molchanov et al. - 2014 - Variational Dropout Sparsifies Deep Neural Networks.pdf:pdf},
number = {Icml},
title = {{Variational Dropout Sparsifies Deep Neural Networks}},
year = {2014}
}
@article{Fabius2015,
abstract = {In this paper we propose a model that combines the strengths of RNNs and SGVB: the Variational Recurrent Auto-Encoder (VRAE). Such a model can be used for efficient, large scale unsupervised learning on time series data, mapping the time series data to a latent vector representation. The model is generative, such that data can be generated from samples of the latent space. An important contribution of this work is that the model can make use of unlabeled data in order to facilitate supervised training of RNNs by initialising the weights and network state.},
archivePrefix = {arXiv},
arxivId = {1412.6581},
author = {Fabius, Otto and van Amersfoort, Joost R.},
eprint = {1412.6581},
file = {:Users/alireza/Library/Application Support/Mendeley Desktop/Downloaded/Fabius, van Amersfoort - 2015 - Variational Recurrent Auto-Encoders.pdf:pdf},
journal = {Iclr},
number = {2013},
pages = {1--5},
title = {{Variational Recurrent Auto-Encoders}},
url = {http://arxiv.org/abs/1412.6581},
year = {2015}
}
