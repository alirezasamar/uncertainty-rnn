Automatically generated by Mendeley Desktop 1.17.9
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{Pulver2016,
abstract = {LSTM is arguably the most successful RNN architecture for many tasks that involve sequential information. In the past few years there have been several proposed improvements to LSTM. We propose an improvement to LSTM which allows communication between memory cells in different blocks and allows an LSTM layer to carry out internal computation within its memory.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.01988},
author = {Pulver, Andrew and Lyu, Siwei},
eprint = {1605.01988},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pulver, Lyu - 2016 - LSTM with Working Memory.pdf:pdf},
title = {{LSTM with Working Memory}},
url = {http://arxiv.org/abs/1605.01988},
year = {2016}
}
@article{Jaques2016,
abstract = {Sequence models can be trained using supervised learning and a next-step prediction objective. This approach, however, suffers from known failure modes. For example, it is notoriously difficult to ensure multi-step generated sequences have coherent global structure. Motivated by the fact that reinforcement learning (RL) can be used to impose arbitrary properties on generated data by choosing appropriate reward functions, in this paper we propose a novel approach for sequence training which combines Maximum Likelihood (ML) and RL training. We refine a sequence predictor by optimizing for some imposed reward functions, while maintaining good predictive properties learned from data. We propose efficient ways to solve this by augmenting deep Q-learning with a cross-entropy reward and deriving novel off-policy methods for RNNs from stochastic optimal control (SOC). We explore the usefulness of our approach in the context of music generation. An LSTM is trained on a large corpus of songs to predict the next note in a musical sequence. This Note-RNN is then refined using RL, where the reward function is a combination of rewards based on rules of music theory, as well as the output of another trained Note-RNN. We show that by combining ML and RL, this RL Tuner method can not only produce more pleasing melodies, but that it can significantly reduce unwanted behaviors and failure modes of the RNN.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.02796},
author = {Jaques, Natasha and Gu, Shixiang and Turner, Richard E. and Eck, Douglas},
eprint = {1611.02796},
isbn = {9783901608353},
journal = {Thesis},
keywords = {Folder - learning - recurrent NN,ear matrix inequality,integral quadratic constraint,lin,recurrent neural network,reinforcement learning,stability analysis},
pages = {410--420},
title = {{Tuning Recurrent Neural Networks with Reinforcement Learning}},
url = {http://repositorium.ub.uni-osnabrueck.de/bitstream/urn:nbn:de:gbv:700-2008112111/2/E-Diss839{\%}7B{\_}{\%}7Dthesis.pdf{\%}5Cnhttp://arxiv.org/abs/1611.02796},
year = {2016}
}
@article{Cvpr2017,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1703.01827},
author = {Cvpr, Anonymous and Id, Paper},
eprint = {1703.01827},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Cvpr, Id - 2017 - All You Need is Beyond a Good Init Exploring Better Solution for Training Extremely Deep Convolutional Neural Network.pdf:pdf},
title = {{All You Need is Beyond a Good Init : Exploring Better Solution for Training Extremely Deep Convolutional Neural Networks with Orthonormality and Modulation}},
year = {2017}
}
@article{Mikolov2015,
abstract = {Recurrent neural network is a powerful model that learns temporal patterns in sequential data. For a long time, it was believed that recurrent networks are difficult to train using simple optimizers, such as stochastic gradient descent, due to the so-called vanishing gradient problem. In this paper, we show that learning longer term patterns in real data, such as in natural language, is perfectly possible using gradient descent. This is achieved by using a slight structural modification of the simple recurrent neural network architecture. We encourage some of the hidden units to change their state slowly by making part of the recurrent weight matrix close to identity, thus forming a kind of longer term memory. We evaluate our model on language modeling tasks on benchmark datasets, where we obtain similar performance to the much more complex Long Short Term Memory (LSTM) networks (Hochreiter {\&} Schmidhuber, 1997).},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7753v1},
author = {Mikolov, Tomas and Joulin, Armand and Chopra, Sumit and Mathieu, Michael},
eprint = {arXiv:1412.7753v1},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Mikolov et al. - 2015 - Learning Longer Memory in Recurrent Neural Networks.pdf:pdf},
journal = {Iclr},
pages = {1--9},
title = {{Learning Longer Memory in Recurrent Neural Networks}},
url = {http://arxiv.org/pdf/1412.7753v1.pdf},
year = {2015}
}
@article{Li2016,
abstract = {Recurrent neural networks (RNN), especially the ones requiring extremely long term memories, are difficult to training. Hence, they provide an ideal testbed for benchmarking the performance of optimization algorithms. This paper reports test results of a recently proposed preconditioned stochastic gradient descent (PSGD) algorithm on RNN training. We find that PSGD may outperform Hessian-free optimization which achieves the state-of-the-art performance on the target problems, although it is only slightly more complicated than stochastic gradient descent (SGD) and is user friendly, virtually a tuning free algorithm.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.04449},
author = {Li, Xi-Lin},
eprint = {1606.04449},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Li - 2016 - Recurrent neural network training with preconditioned stochastic gradient descent.pdf:pdf},
journal = {arXiv},
keywords = {Optimization,PSGD,Precondition,RNN},
number = {1},
pages = {1--9},
title = {{Recurrent neural network training with preconditioned stochastic gradient descent}},
url = {http://arxiv.org/abs/1606.04449},
year = {2016}
}
@article{Dieng2016,
abstract = {In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based language model designed to directly capture the global semantic meaning relating words in a document via latent topics. Because of their sequential nature, RNNs are good at capturing the local structure of a word sequence - both semantic and syntactic - but might face difficulty remembering long-range dependencies. Intuitively, these long-range dependencies are of semantic nature. In contrast, latent topic models are able to capture the global underlying semantic structure of a document but do not account for word ordering. The proposed TopicRNN model integrates the merits of RNNs and latent topic models: it captures local (syntactic) dependencies using an RNN and global (semantic) dependencies using latent topics. Unlike previous work on contextual RNN language modeling, our model is learned end-to-end. Empirical results on word prediction show that TopicRNN outperforms existing contextual RNN baselines. In addition, TopicRNN can be used as an unsupervised feature extractor for documents. We do this for sentiment analysis and report a new state-of-the-art error rate on the IMDB movie review dataset that amounts to a {\$}13.3\backslash{\%}{\$} improvement over the previous best result. Finally TopicRNN also yields sensible topics, making it a useful alternative to document models such as latent Dirichlet allocation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.01702},
author = {Dieng, Adji B. and Wang, Chong and Gao, Jianfeng and Paisley, John},
eprint = {1611.01702},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dieng et al. - 2016 - TopicRNN A Recurrent Neural Network with Long-Range Semantic Dependency.pdf:pdf},
isbn = {9783901608353},
pages = {1--11},
title = {{TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency}},
url = {http://arxiv.org/abs/1611.01702},
year = {2016}
}
@article{Sak2014,
abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1402.1128},
author = {Sak, Ha≈üim and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
doi = {arXiv:1402.1128},
eprint = {1402.1128},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Sak, Senior, Beaufays - 2014 - Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recogniti.pdf:pdf},
journal = {Interspeech},
number = {Cd},
pages = {338--342},
title = {{Long Short-Term Memory Based Recurrent Neural Network Architectures for Large Vocabulary Speech Recognition}},
url = {http://arxiv.org/abs/1402.1128},
year = {2014}
}
@article{Hochreiter1997,
annote = {NULL},
author = {{Hochreiter, Sepp; Schmidhuber}, Jurgen},
doi = {10.1144/GSL.MEM.1999.018.01.02},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Hochreiter - 1997 - 2 Previous Work.pdf:pdf},
isbn = {9781457711022},
issn = {0435-4052},
journal = {Neural Computation},
keywords = {LSTM Intro,Previous Work},
mendeley-tags = {LSTM Intro,Previous Work},
number = {8},
pages = {1--32},
title = {{Long short-term memory}},
volume = {9},
year = {1997}
}
@article{Woodward2016,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1702.06559v1},
author = {Woodward, Mark and Finn, Chelsea},
eprint = {arXiv:1702.06559v1},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Woodward, Finn - 2016 - Active One-shot Learning.pdf:pdf},
title = {{Active One-shot Learning}},
year = {2016}
}
@article{Chung2014,
abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1412.3555v1},
author = {Chung, Junyoung and Gulcehre, Caglar and Cho, Kyunghyun and Bengio, Yoshua},
eprint = {1412.3555v1},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chung et al. - 2014 - Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling.pdf:pdf},
journal = {arXiv},
pages = {1--9},
title = {{Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling}},
year = {2014}
}
@article{Che2016,
abstract = {Many multivariate time series data in practical applications, such as health care, geoscience, and biology, are characterized by a variety of missing values. It has been noted that the missing patterns and values are often correlated with the target labels, a.k.a., missingness is informative, and there is significant interest to explore methods which model them for time series prediction and other related tasks. In this paper, we develop novel deep learning models based on Gated Recurrent Units (GRU), a state-of-the-art recurrent neural network, to handle missing observations. Our model takes two representations of missing patterns, i.e., masking and time duration, and effectively incorporates them into a deep model architecture so that it not only captures the long-term temporal dependencies in time series, but also utilizes the missing patterns to improve the prediction results. Experiments of time series classification tasks on real-world clinical datasets (MIMIC-III, PhysioNet) and synthetic datasets demonstrate that our models achieve state-of-art performance on these tasks and provide useful insights for time series with missing values.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.01865},
author = {Che, Zhengping and Purushotham, Sanjay and Cho, Kyunghyun and Sontag, David and Liu, Yan},
eprint = {1606.01865},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Che et al. - 2016 - Recurrent Neural Networks for Multivariate Time Series with Missing Values.pdf:pdf},
journal = {arXiv:1606.01865v1 [cs.LG]},
pages = {1--14},
title = {{Recurrent Neural Networks for Multivariate Time Series with Missing Values}},
year = {2016}
}
@article{Maheshwari2012,
annote = {NULL},
author = {Maheshwari, Gaurav and Pudi, Vikram},
isbn = {9782874190490},
number = {April},
pages = {25--27},
title = {{RNN based Batch Mode Active Learning Framework}},
year = {2012}
}
@misc{Bengio1994,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Bengio, Y},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Bengio - 1994 - Learning long-term dependencies with gradient descent is difficult.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
keywords = {Problem Statement},
mendeley-tags = {Problem Statement},
pmid = {18267787},
title = {{Learning long-term dependencies with gradient descent is difficult}},
year = {1994}
}
@article{Chernodub2016,
abstract = {Vanishing (and exploding) gradients effect is a common problem for recurrent neural networks with nonlinear activation functions which use backpropagation method for calculation of derivatives. Deep feedforward neural networks with many hidden layers also suffer from this effect. In this paper we propose a novel universal technique that makes the norm of the gradient stay in the suitable range. We construct a way to estimate a contribution of each training example to the norm of the long-term components of the target function s gradient. Using this subroutine we can construct mini-batches for the stochastic gradient descent (SGD) training that leads to high performance and accuracy of the trained network even for very complex tasks. We provide a straightforward mathematical estimation of minibatch s impact on for the gradient norm and prove its correctness theoretically. To check our framework experimentally we use some special synthetic benchmarks for testing RNNs on ability to capture long-term dependencies. Our network can detect links between events in the (temporal) sequence at the range approx. 100 and longer.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1606.07767},
author = {Chernodub, Artem and Nowicki, Dimitri},
doi = {10.1007/978-3-319-46672-9},
eprint = {1606.07767},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Chernodub, Nowicki - 2016 - Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks.pdf:pdf},
isbn = {9783319466729},
title = {{Sampling-based Gradient Regularization for Capturing Long-Term Dependencies in Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1606.07767},
year = {2016}
}
@article{Dorobantu1994,
abstract = {The vanishing and exploding gradient prob-lems are well-studied obstacles that make it difficult for recurrent neural networks to learn long-term time dependencies. We pro-pose a reparameterization of standard recur-rent neural networks to update linear trans-formations in a provably norm-preserving way through Givens rotations. Addition-ally, we use the absolute value function as an element-wise non-linearity to preserve the norm of backpropagated signals over the en-tire network. We show that this reparame-terization reduces the number of parameters and maintains the same algorithmic complex-ity as a standard recurrent neural network, while outperforming standard recurrent neu-ral networks with orthogonal initializations and Long Short-Term Memory networks on the copy problem.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.04035},
author = {Dorobantu, Victor and Stromhaug, Andre and Renteria, Jess},
eprint = {1612.04035},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dorobantu, Stromhaug, Renteria - 1994 - DizzyRNN Reparameterizing Recurrent Neural Networks for Norm-Preserving Backpropagation.pdf:pdf},
title = {{DizzyRNN: Reparameterizing Recurrent Neural Networks for Norm-Preserving Backpropagation}},
year = {1994}
}
@article{Pacanu2013,
abstract = {Problem Statement},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {arXiv:1211.5063v2},
author = {Pacanu, R and Mikolov, T and Bengio, Y},
doi = {10.1109/72.279181},
eprint = {arXiv:1211.5063v2},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Pacanu, Mikolov, Bengio - 2013 - On the Difficulties of Training Recurrent Neural Networks.pdf:pdf},
isbn = {1045-9227 VO - 5},
issn = {19410093},
journal = {Icml},
number = {2},
pmid = {18267787},
title = {{On the Difficulties of Training Recurrent Neural Networks}},
year = {2013}
}
@article{Collins2016,
abstract = {Two potential bottlenecks on the expressiveness of recurrent neural networks (RNNs) are their ability to store information about the task in their parameters, and to store information about the input history in their units. We show experimentally that all common RNN architectures achieve nearly the same per task and per unit capacity bounds with careful training, for a variety of tasks and stacking depths. They can store approximately 5 bits of task information per parameter, and one real number per unit. We further find that for several tasks it is the per-task parameter capacity bound that determines performance. These results suggest that many previous results comparing RNN architectures are driven primarily by differences in training effectiveness, rather than differences in capacity. Supporting this observation, we compare training difficulty for several architectures, and show that vanilla RNNs are far more difficult to train, yet have higher capacity. Finally, we propose two novel RNN architectures, one of which is easier to train than the LSTM or GRU.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1611.09913},
author = {Collins, Jasmine and Sohl-Dickstein, Jascha and Sussillo, David},
eprint = {1611.09913},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Collins, Sohl-Dickstein, Sussillo - 2016 - Capacity and trainability in recurrent neural networks.pdf:pdf},
journal = {arXiv},
pages = {1--16},
title = {{Capacity and trainability in recurrent neural networks}},
year = {2016}
}
@article{Glorot2010,
abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We Ô¨Årst observe the inÔ¨Çuence of the non-linear activations functions. We Ô¨Ånd that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we Ô¨Ånd that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We Ô¨Ånd that a new non-linearity that saturates less can often be beneÔ¨Åcial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difÔ¨Åcult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
annote = {NULL},
author = {Glorot, Xavier and Bengio, Yoshua},
doi = {10.1.1.207.2059},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Glorot, Bengio - 2010 - Understanding the difficulty of training deep feedforward neural networks.pdf:pdf},
isbn = {9781937284275},
issn = {15324435},
journal = {Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS)},
pages = {249--256},
title = {{Understanding the difficulty of training deep feedforward neural networks}},
url = {http://machinelearning.wustl.edu/mlpapers/paper{\_}files/AISTATS2010{\_}GlorotB10.pdf},
volume = {9},
year = {2010}
}
@article{Le2016,
abstract = {Recursive neural networks (RNN) and their recently proposed extension recursive long short term memory networks (RLSTM) are models that compute representations for sentences, by recursively combining word embeddings according to an externally provided parse tree. Both models thus, unlike recurrent networks, explicitly make use of the hierarchical structure of a sentence. In this paper, we demonstrate that RNNs nevertheless suffer from the vanishing gradient and long distance dependency problem, and that RLSTMs greatly improve over RNN's on these problems. We present an artificial learning task that allows us to quantify the severity of these problems for both models. We further show that a ratio of gradients (at the root node and a focal leaf node) is highly indicative of the success of backpropagation at optimizing the relevant weights low in the tree. This paper thus provides an explanation for existing, superior results of RLSTMs on tasks such as sentiment analysis, and suggests that the benefits of including hierarchical structure and of including LSTM-style gating are complementary.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1603.00423},
author = {Le, Phong and Zuidema, Willem},
eprint = {1603.00423},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Le, Zuidema - 2016 - Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive.pdf:pdf},
journal = {ACL WS on on Representation Learning for NLP},
number = {2010},
pages = {87--93},
title = {{Quantifying the vanishing gradient and long distance dependency problem in recursive neural networks and recursive LSTMs}},
url = {http://arxiv.org/abs/1603.00423},
year = {2016}
}
@article{Lipton2015,
abstract = {We present a novel application of LSTM recurrent neural networks to multilabel classification of diagnoses given variable-length time series of clinical measurements. Our method outperforms a strong baseline on a variety of metrics.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1510.07641},
author = {Lipton, Zachary C. and Kale, David C. and Wetzell, Randall C.},
eprint = {1510.07641},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Lipton, Kale, Wetzell - 2015 - Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks.pdf:pdf},
isbn = {9781450336642},
journal = {Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {electronic health records,regularization,temporal graph,temporal phenotyping},
pages = {705--714},
title = {{Phenotyping of Clinical Time Series with LSTM Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1510.07641},
year = {2015}
}
@article{Dey,
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1701.05923},
author = {Dey, Rahul and Salem, Fathi M},
eprint = {1701.05923},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Dey, Salem - Unknown - Gate-Variants of Gated Recurrent Unit ( GRU ) Neural Networks.pdf:pdf},
title = {{Gate-Variants of Gated Recurrent Unit ( GRU ) Neural Networks}},
volume = {784}
}
@article{Zaremba2014,
abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1409.2329},
author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
doi = {ng},
eprint = {1409.2329},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Zaremba, Sutskever, Vinyals - 2014 - Recurrent Neural Network Regularization.pdf:pdf},
isbn = {078036404X},
issn = {0157244X},
journal = {Iclr},
keywords = {Natural Language Processing, Recurrent Neural Netw,Recurrent Neural Netw},
number = {2013},
pages = {1--8},
title = {{Recurrent Neural Network Regularization}},
url = {http://arxiv.org/abs/1409.2329},
year = {2014}
}
@article{Laurent2016,
abstract = {We introduce an exceptionally simple gated recurrent neural network (RNN) that achieves performance comparable to well-known gated architectures, such as LSTMs and GRUs, on the word-level language modeling task. We prove that our model has simple, predicable and non-chaotic dynamics. This stands in stark contrast to more standard gated architectures, whose underlying dynamical systems exhibit chaotic behavior.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1612.06212},
author = {Laurent, Thomas and von Brecht, James},
eprint = {1612.06212},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Laurent, von Brecht - 2016 - A recurrent neural network without chaos.pdf:pdf},
number = {1},
pages = {1--11},
title = {{A recurrent neural network without chaos}},
url = {http://arxiv.org/abs/1612.06212},
year = {2016}
}
@article{Maheshwari2011,
annote = {NULL},
author = {Maheshwari, Gaurav and Vinzamuri, Bhanukiran and Pudi, Vikram},
journal = {International Conference on Machine Learning and Data Mining (MLDM 2011)},
title = {{RNN based Sampling Technique for Effective Active Learning}},
year = {2011}
}
@article{Santoro2016,
abstract = {Despite recent breakthroughs in the applications of deep neural networks, one setting that presents a persistent challenge is that of "one-shot learning." Traditional gradient-based networks require a lot of data to learn, often through extensive iterative training. When new data is encountered, the models must inefficiently relearn their parameters to adequately incorporate the new information without catastrophic interference. Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models. Here, we demonstrate the ability of a memory-augmented neural network to rapidly assimilate new data, and leverage this data to make accurate predictions after only a few samples. We also introduce a new method for accessing an external memory that focuses on memory content, unlike previous methods that additionally use memory location-based focusing mechanisms.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1605.06065},
author = {Santoro, Adam and Bartunov, Sergey and Botvinick, Matthew and Wierstra, Daan and Lillicrap, Timothy},
doi = {10.1002/2014GB005021},
eprint = {1605.06065},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Santoro et al. - 2016 - One-shot Learning with Memory-Augmented Neural Networks.pdf:pdf},
isbn = {9781617796029},
issn = {19449224},
pmid = {8190083},
title = {{One-shot Learning with Memory-Augmented Neural Networks}},
year = {2016}
}
@article{Irie2016,
abstract = {Popularized by the long short-term memory (LSTM), multi-plicative gates have become a standard means to design arti-ficial neural networks with intentionally organized information flow. Notable examples of such architectures include gated re-current units (GRU) and highway networks. In this work, we first focus on the evaluation of each of the classical gated ar-chitectures for language modeling for large vocabulary speech recognition. Namely, we evaluate the highway network, lateral network, LSTM and GRU. Furthermore, the motivation under-lying the highway network also applies to LSTM and GRU. An extension specific to the LSTM has been recently proposed with an additional highway connection between the memory cells of adjacent LSTM layers. In contrast, we investigate an approach which can be used with both LSTM and GRU: a highway net-work in which the LSTM or GRU is used as the transformation function. We found that the highway connections enable both standalone feedforward and recurrent neural language models to benefit better from the deep structure and provide a slight improvement of recognition accuracy after interpolation with count models. To complete the overview, we include our initial investigations on the use of the attention mechanism for learn-ing word triggers.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1503.0075},
author = {Irie, Kazuki and T??ske, Zolt??n and Alkhouli, Tamer and Schl??ter, Ralf and Ney, Hermann},
doi = {10.21437/Interspeech.2016-491},
eprint = {1503.0075},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Irie et al. - 2016 - LSTM, GRU, highway and a bit of attention An empirical overview for language modeling in speech recognition.pdf:pdf},
isbn = {9781937284244},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Attention,Gated recurrent unit,Highway network,Language modeling,Long short-term memory,Speech recognition,Word trigger},
number = {September},
pages = {3519--3523},
pmid = {18267787},
title = {{LSTM, GRU, highway and a bit of attention: An empirical overview for language modeling in speech recognition}},
volume = {08-12-Sept},
year = {2016}
}
@article{Neil2016,
abstract = {Recurrent Neural Networks (RNNs) have become the state-of-the-art choice for extracting patterns from temporal sequences. Current RNN models are ill suited to process irregularly sampled data triggered by events generated in continuous time by sensors or other neurons. Such data can occur, for example, when the input comes from novel event-driven artificial sensors which generate sparse, asynchronous streams of events or from multiple conventional sensors with different update intervals. In this work, we introduce the Phased LSTM model, which extends the LSTM unit by adding a new time gate. This gate is controlled by a parametrized oscillation with a frequency range which require updates of the memory cell only during a small percentage of the cycle. Even with the sparse updates imposed by the oscillation, the Phased LSTM network achieves faster convergence than regular LSTMs on tasks which require learning of long sequences. The model naturally integrates inputs from sensors of arbitrary sampling rates, thereby opening new areas of investigation for processing asynchronous sensory events that carry timing information. It also greatly improves the performance of LSTMs in standard RNN applications, and does so with an order-of-magnitude fewer computes.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1610.09513},
author = {Neil, Daniel and Pfeiffer, Michael and Liu, Shih-Chii},
eprint = {1610.09513},
file = {:home/alireza/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Neil, Pfeiffer, Liu - 2016 - Phased LSTM Accelerating Recurrent Network Training for Long or Event-based Sequences.pdf:pdf},
journal = {Nips},
number = {Nips},
pages = {3882--3890},
title = {{Phased LSTM: Accelerating Recurrent Network Training for Long or Event-based Sequences}},
url = {https://papers.nips.cc/paper/6310-phased-lstm-accelerating-recurrent-network-training-for-long-or-event-based-sequences},
year = {2016}
}
