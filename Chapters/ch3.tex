\chapter{Research Methodology}
\label{chap:method}

\section{Research Activities}



\section{Technique: Backprop Through Time}
\label{sec:bptt}

As illustrated in \ref{fig:rnn-rolled}, the core of an recurrent neural networks (RNNs), $f$, is a neural network that maps the RNN state at step $t$, $s_t$ and an input observation $x_t$ to a new RNN state $s_{t+1}$, $f: (s_t, x_t) \mapsto s_{t+1}$.

For comparison, an LSTM-powered RNN core \cite{Hochreiter1997} has a state $s_t = (c_t, h_t)$ where $c$ is an internal core state and $h$ is the exposed state. Intermediate gates modulate the effect of the inputs on the outputs, gates like the input gate $i_t$, forget gate $f_t$ and output gate $o_t$. The correlation between the inputs, outputs and internal gates of an LSTM cell can be explained as:

\begin{align*}
i_t &= \sigma(W_i [x_t, h_{t-1}]^T + b_i), \\
f_t &= \sigma(W_f [x_t, h_{t-1}]^T + b_f), \\
c_t &= f_t c_{t-1} + i_t \tanh(W_c [x_t, h_{t-1}] + b_c), \\
o_t &= \sigma(W_o [x_t, h_{t-1}]^T + b_o), \\
h_t &= o_t \tanh(c_t),
\end{align*}

In the above statement, $W_i$ ($b_i$), $W_f$ ($b_f$), $W_c$ ($b_c$) and $W_o$ ($b_o$) are the weights (biases) that are affecting the input gate, forget gate, cell update, and output gate accordingly.

An RNN can be trained on a sequence of $T$ using backpropagation through time where the RNN is unrolled $T$ times like a feed-forward network.
Which can be achieve with forming the feed-forward network with inputs
$x_1, x_2, \dots, x_T$ and initial state $s_0$:

\begin{align}
s_1 &= f(s_0, x_1), \nonumber \\
s_2 &= f(s_1, x_2), \nonumber \\
&\dots \nonumber \\
\label{eq:unroll}
s_T &= f(s_{T-1}, x_T), 
\end{align}

where $s_T$ is the total length (final state) of the RNN.
Referring to the unrolled RNN for $T$ steps as in \eqref{eq:unroll} by $s_{1:T} = F_T(x_{1:T}, s_0)$,
where $x_{1:T}$ is the sequence of input vectors and $s_{1:T}$ is the sequence of corresponding states. However, the truncated version of the algorithm can be seen as taking $s_0$ as the last state of the previous batch, $s_T$.

This RNN parameters are trained in the same way as a feed-forward neural network and a loss is applied to the states $s_{1:T}$ of the RNN, and then backpropagation is being used to update the weights of the trained network.
Since the weights in each of the unrolled step are shared, each weight of the RNN core receives $T$ gradient contributions when the it is unrolled for $T$ steps.

\section{Benchmarks}

\subsection{Language Modeling}
The experiments result of this work will be tested in Chapter 5, on the Penn Treebank \cite{Marcus1993} benchmark, a task consisting on next word prediction. Using the network architecture from \cite{Zaremba2014} as a baseline and for which there is an open source implementation\footnote{\url{https://github.com/tensorflow/models/blob/master/tutorials/rnn/ptb/ptb_word_lm.py}}.

\begin{table}[t]
	\caption{Word-level perplexity on the Penn Treebank language modelling task (lower is better).}
	\label{tab:ptb}
	\vskip 0.15in
	\centering
	\small
	\begin{tabular}{r|c|c}
		\textbf{Model (medium)}             & \textbf{Validation} & \textbf{Test} \\ 
		\hline
		\abovespace
		LSTM & 120.7 & 114.5\\
		LSTM dropout                        & 	86.2	        &	82.1 \\
		SGLD    	                        &	N/A				&	109.1		\\
		SGLD dropout	                    &	N/A				&	99.6		\\
		Variational LSTM (tied weights)     &	81.8        	&	79.7 \\
		Variational LSTM (tied weights, MS)	&	N/A		        &	79.0		\\
		\hline 
		& &  \\
		\shortstack{\textbf{Model (medium)} \\ \textbf{with Dynamic Evaluation}}
		& \textbf{Validation} & \textbf{Test} \\
		\hline
		\abovespace
		LSTM dropout                      & 	79.7	        &	77.1 \\
		\hline
	\end{tabular}
	\vskip -0.1in
\end{table}

A vast report on recent advances on the Penn Treebank benchmark \cite{Kim2016}, \cite{Zilly2016}, \cite{Merity2016} that shows achievement of perplexities of 70.9 on the test set.

\section{Chapter Summary}