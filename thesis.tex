\documentclass{utmthesis}
\usepackage{graphicx}
\usepackage{url} 
\usepackage[pages=some]{background}
\usepackage{algorithm,algorithmic}
\usepackage{algpseudocode}
\usepackage{amsmath}

\begin{document}

% Required information
\title{Uncertainty in Recurrent Neural Networks}
\author{Alireza Samar}
\degree{Master of Philosophy}
\specialization{Machine Learning}
\intakeyear{2016}
\faculty{Advanced Informatics School}
\titledate{April 2017}
\award{4}
% Options for Award 
% 1. Bachelor Degree Project Report
% 2. Master's Project Report (By course work)
% 3. Master's Dissertation (By course work and research)
% 4. Master's Thesis (By research)
% 5. Doctor of Philosophy Thesis
% 6. Other PhD Thesis
% 7. Generic PhD Thesis
% 8. Thesis Proposal
\superone{Dr. Siti Sophiayati Yuhaniz}
%\supertwo{M.Y. Other Supervisor}
%\superthree{Third SV}
%\superfour{Fourth SV}
%\superfive{Fifth SV}

% Option for two-page printing
\newgeometry{top=2.5cm,left=4cm,right=2.5cm,bottom=2.5cm,twoside}

% Option to add watermark page
% Comment for final version	
\backgroundsetup{scale=1,angle=0,opacity=.1,hshift=0.25in,vshift=-0.5in,contents={\includegraphics[width=5cm]{figs/utm02.jpg}}}
% \watermarkpage

% Mandatory pages
%\coverpage
%\superpage
%\certification
%\frontmatter
\maketitle
%\declaration

%\begin{dedication}
%Dedication\
%\end{dedication}

%\begin{acknowledgement}
%Acknowledgement
%\end{acknowledgement}

\begin{abstract}
Deep learning has outperformed in various fields from computer vision, and language processing to physics, biology, and manufacturing. This means the deep or multi-layer architecture of neural networks are being extensively used in these fields; for instance convolutional neural networks (CNN) as image processing tools, and recurrent neural networks (RNN) as sequence processing model.

However, in traditional sciences fields such as physics and biology, model uncertainty is crucial, especially in time series models where delay canâ€™t be tolerated. In this work, I aim to propose a novel theoretical framework and develop tools to measure uncertainty estimates, especially in deep recurrent neural networks.

This work also tackles a widely known difficulty of training recurrent neural networks, vanishing gradient by proposing a novel architecture of RNN that compute weighted average unit on past iteration.

\end{abstract}

%\begin{abstrak}
%Ini adalah abstrak Bahasa Melayu
%\end{abstrak}

\tableofcontents
\listoftables
\listoffigures

%List of abbreviation 
\listofabbre
\addabbre{ANN}{Artificial Neural Network}
\addabbre{RNN}{Recurrent Neural Network}
\addabbre{BBB}{Bayes by Backprop}
\addabbre{CNN}{Convolutional Neural Network}
\addabbre{KL}{Kullback-Leibler}

%List of symbols 
\listofsymbols
\addsymbol{$\gamma$}{Whatever}
\addsymbol{$\sigma$}{Whatever}
\addsymbol{$\varepsilon$}{Whatever}

%List of appendices
%\listofappendices

\onehalfspacing
\mainmatter

\include{Chapters/ch1}
\include{Chapters/ch2}
\include{Chapters/ch3}
\include{Chapters/ch4}
%\include{Chapters/ch5}
%\include{Chapters/ch6}

\bibliographystyle{utmthesis-numbering}
\bibliography{reference}

%\appendix
%\chapter{Do not use long titles.}
%\chapter{Pseudo-codes}
%\chapter{Time-series Results}

%This is required to make List of Appendices possible. Remove when have no appendix.
%\endmatter
\end{document}
